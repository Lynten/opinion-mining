{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 智能跑鞋商品评论挖掘分析（以天猫商场为例）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在天猫上搜索“智能跑鞋”，共可得500+件商品。爬取了这500多个智能跑鞋的商品信息和用户评论信息，存储在data文件夹内。\n",
    "\n",
    "其中使用了多线程技术。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#_*_coding:utf-8_*_\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import codecs\n",
    "import threading\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# 工作文件夹\n",
    "WORKSPACE = r'G:/Data/NetEase/'\n",
    "\n",
    "# 基本配置\n",
    "import matplotlib.font_manager as fm\n",
    "fp_msyh = fm.FontProperties(fname=WORKSPACE+'msyh.ttc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 预处理通用函数\n",
    "from six.moves.html_parser import HTMLParser\n",
    "def preprocess(text):\n",
    "    if type(text)==str:\n",
    "        text=unicode(text,'utf-8')\n",
    "    re_h=re.compile('</?\\w+[^>]*>')# HTML tags\n",
    "    result=re_h.sub('',text)\n",
    "    \n",
    "    result=HTMLParser().unescape(result) # HTML entities\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stanford CoreNLP（分词、词性标注、句法树、依存关系）\n",
    "def stanford_nlp(text):\n",
    "    properties={'pinelineLanguage':'zh','annotators': 'tokenize,ssplit,pos,depparse','outputFormat': 'json'}\n",
    "    \n",
    "    r=requests.post('http://localhost:9000',params={'properties': str(properties)},data=text)  \n",
    "    r_json=json.loads(r.text)\n",
    "    \n",
    "    return r_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据爬取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬取商品信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-06894a34dfea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[0mshoes_df_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mshoes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m     \u001b[0mshoes_df_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_all_product\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mout_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshoes_df_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'out_path' is not defined"
     ]
    }
   ],
   "source": [
    "def get_all_product(query,out_path):\n",
    "    \n",
    "    base_url = r'https://list.tmall.com/m/search_items.htm?page_size=50&page_no={}&q={}'\n",
    "    \n",
    "    r_json = json.loads(requests.get(base_url.format(0,query)).text)\n",
    "    total_page = r_json['total_page']\n",
    "    total_num = r_json['total_results']\n",
    "    print 'Total page: {} Total num: {}'.format(total_page, total_num)\n",
    "\n",
    "    item_ids = []\n",
    "    cat_ids = []\n",
    "    sku_ids = []\n",
    "    spu_ids = []\n",
    "\n",
    "    comment_nums = []\n",
    "    solds = []\n",
    "\n",
    "    is_promotions = []\n",
    "    prices = []\n",
    "    post_fees = []\n",
    "\n",
    "    seller_ids = []\n",
    "    seller_locs = []\n",
    "    shop_names = []\n",
    "\n",
    "    titles = []\n",
    "\n",
    "    for page in xrange(total_page):\n",
    "        url = base_url.format(page,query)\n",
    "\n",
    "        try:\n",
    "            r_json = json.loads(requests.get(url).text)\n",
    "        except:\n",
    "            print 'Error!', url\n",
    "            continue\n",
    "\n",
    "        for item in r_json['item']:\n",
    "            item_ids.append(item['item_id'])\n",
    "            cat_ids.append(item['cat_id'])\n",
    "            sku_ids.append(item['sku_id'])\n",
    "            spu_ids.append(item['spu_id'])\n",
    "\n",
    "            comment_nums.append(item['comment_num'])\n",
    "            solds.append(item['sold'])\n",
    "\n",
    "            is_promotions.append(item['is_promotion'])\n",
    "            prices.append(item['price'])\n",
    "            post_fees.append(item['post_fee'])\n",
    "\n",
    "            seller_ids.append(item['seller_id'])\n",
    "            seller_locs.append(item['seller_loc'])\n",
    "            shop_names.append(item['shop_name'])\n",
    "\n",
    "            titles.append(item['title'])\n",
    "\n",
    "    df_product = pd.DataFrame({'item_id': item_ids, 'cat_id': cat_ids, 'sku_id': sku_ids, 'spu_id': spu_ids, 'comment_num': comment_nums, 'sold': solds,\n",
    "                               'is_promotion': is_promotions, 'price': prices, 'post_fee': post_fees, 'seller_id': seller_ids, 'seller_loc': seller_locs, 'shop_name': shop_names, 'title': titles}, columns=['item_id', 'cat_id', 'sku_id', 'spu_id', 'comment_num', 'sold', 'is_promotion', 'price', 'post_fee', 'seller_id', 'seller_loc', 'shop_name', 'title'])\n",
    "    df_product=df_product.drop_duplicates()\n",
    "#     df_product.to_csv(out_path, encoding='utf-8', index=False)\n",
    "    print 'Size {}. Saved at {}.'.format(df_product.shape,out_path)\n",
    "    return df_product\n",
    "\n",
    "# out_path=WORKSPACE + 'data/product.csv'\n",
    "# get_all_product('智能跑鞋',out_path)\n",
    "\n",
    "shoes=['鞋子','鞋子 男','鞋子 女','板鞋','布鞋','皮鞋','帆布鞋','运动鞋','休闲鞋']\n",
    "shoes_df_list=[]\n",
    "for s in shoes:\n",
    "    shoes_df_list.append(get_all_product(s,out_path))\n",
    "df=pd.concat(shoes_df_list).drop_duplicates()\n",
    "print df.shape\n",
    "df.to_csv(WORKSPACE + 'data/product_shoes.csv', encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬取商品评论\n",
    "order: 1：按时间排序；3：默认排序\n",
    "\n",
    "append: 0 or 1， 切换追加评论\n",
    "\n",
    "content: 0 or 1， 是否显示有内容的评论\n",
    "\n",
    "needFold: 0 or 1， 是否折叠评论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crawl_one_item(item_id, seller_id):\n",
    "\n",
    "    #     item_id=531979715453\n",
    "    #     seller_id=1770496755\n",
    "\n",
    "    comment_url = r'https://rate.tmall.com/list_detail_rate.htm?itemId={}&sellerId={}&order=1&currentPage=1&append=0&content=0'.format(\n",
    "        item_id, seller_id)\n",
    "\n",
    "    r = requests.get(comment_url)\n",
    "    r_json = json.loads(r.text[15:])\n",
    "\n",
    "    page_num = r_json['paginator']['lastPage']\n",
    "    page_num_fold = json.loads(requests.get(\n",
    "        r'https://rate.tmall.com/list_detail_rate.htm?itemId={}&sellerId={}&order=1&currentPage=1&append=0&content=0'.format(item_id, seller_id)).text[15:])['paginator']['lastPage']\n",
    "\n",
    "    rate_dates = []\n",
    "    auction_skus = []\n",
    "    tmall_sweet_levels = []\n",
    "    is_folds = []\n",
    "    contents = []\n",
    "\n",
    "    # 翻页\n",
    "    page = 1\n",
    "    while page <= page_num:\n",
    "        comment_url = r'https://rate.tmall.com/list_detail_rate.htm?itemId={}&sellerId={}&order=1&currentPage={}&append=0&content=0&needFold=0'.format(item_id, seller_id,\n",
    "                                                                                                                                                       page)\n",
    "        r = requests.get(comment_url)\n",
    "\n",
    "        try:\n",
    "            r_json = json.loads(r.text[15:])\n",
    "        except:\n",
    "            print 'Warning: ' + comment_url\n",
    "            sys.stdout.flush()\n",
    "            continue\n",
    "\n",
    "        for comment in r_json['rateList']:\n",
    "\n",
    "            rate_dates.append(comment['rateDate'])\n",
    "            auction_skus.append(comment['auctionSku'])\n",
    "            tmall_sweet_levels.append(comment['tamllSweetLevel'])\n",
    "            is_folds.append(False)\n",
    "            contents.append(comment['rateContent'])\n",
    "\n",
    "        page += 1\n",
    "\n",
    "    # 翻页，被折叠的评论\n",
    "    page = 1\n",
    "    while page <= page_num_fold:\n",
    "        comment_url = r'https://rate.tmall.com/list_detail_rate.htm?itemId={}&sellerId={}&order=1&currentPage={}&append=0&content=0&needFold=1'.format(\n",
    "            item_id, seller_id, page)\n",
    "        r = requests.get(comment_url)\n",
    "\n",
    "        try:\n",
    "            r_json = json.loads(r.text[15:])\n",
    "        except:\n",
    "            print 'Warning: ' + comment_url\n",
    "            sys.stdout.flush()\n",
    "            continue\n",
    "\n",
    "        for comment in r_json['rateList']:\n",
    "            rate_dates.append(comment['rateDate'])\n",
    "            auction_skus.append(comment['auctionSku'])\n",
    "            tmall_sweet_levels.append(comment['tamllSweetLevel'])\n",
    "            is_folds.append(True)\n",
    "            contents.append(comment['rateContent'])\n",
    "\n",
    "        page += 1\n",
    "\n",
    "    # Save\n",
    "    df_comment = pd.DataFrame({'rate_date': rate_dates, 'auction_sku': auction_skus, 'tmall_sweet_level': tmall_sweet_levels,\n",
    "                               'is_fold': is_folds, 'content': contents}, columns=['rate_date', 'auction_sku', 'tmall_sweet_level', 'is_fold', 'content'])\n",
    "    df_comment.to_csv(WORKSPACE + 'data/comment_shoes/' +\n",
    "                      str(item_id) + '.csv', encoding='utf-8', index=False)\n",
    "    print '-> Finished item_id: {}'.format(item_id)\n",
    "\n",
    "# Test\n",
    "# crawl_one_item(531818703657, 363607599)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crawl_part(item_ids,seller_ids):\n",
    "    assert len(item_ids)==len(seller_ids)\n",
    "    \n",
    "    num=len(item_ids)\n",
    "    idx=0\n",
    "    while idx < num:\n",
    "        try:\n",
    "            print 'Crawling: {}'.format(item_ids[idx])\n",
    "            crawl_one_item(item_ids[idx],seller_ids[idx])\n",
    "        except:\n",
    "            continue\n",
    "        idx+=1\n",
    "                       \n",
    "def get_all_comment(df_product):          \n",
    "    item_ids=[]\n",
    "    seller_ids=[]\n",
    "    for ids,row in df_product.sort_values(by='item_id').iterrows():\n",
    "        item_ids.append(row['item_id'])\n",
    "        seller_ids.append(row['seller_id'])\n",
    "\n",
    "    thread_num=20 # The final thread num will plus one when there is remainder\n",
    "    total_num=len(item_ids)\n",
    "    thread_size=total_num/thread_num\n",
    "    print 'Total product items: {}'.format(total_num)\n",
    "\n",
    "    threads=[]\n",
    "    for idx in xrange(thread_num):\n",
    "        start=idx*thread_size\n",
    "        end=(idx+1)*thread_size\n",
    "        t=threading.Thread(target=crawl_part,args=(item_ids[start:end],seller_ids[start:end]))\n",
    "        threads.append(t)\n",
    "\n",
    "    remainder=total_num%thread_num\n",
    "    if not remainder==0:\n",
    "        t=threading.Thread(target=crawl_part,args=(item_ids[-remainder:],seller_ids[-remainder:]))\n",
    "        threads.append(t)\n",
    "\n",
    "    for t in threads:\n",
    "        t.start()\n",
    "\n",
    "    happy_ending=False\n",
    "\n",
    "    while not happy_ending:\n",
    "        active_cout=0\n",
    "        for t in threads:\n",
    "            if t.is_alive():\n",
    "                active_cout+=1   \n",
    "        if active_cout<=0:\n",
    "            happy_ending=True\n",
    "        else:\n",
    "            print '{} threads still running...'.format(active_cout)        \n",
    "        time.sleep(5)\n",
    "\n",
    "    print 'Happy ending!'\n",
    "\n",
    "\n",
    "df_product=pd.read_csv(WORKSPACE +'data/product_shoes.csv')         \n",
    "get_all_comment(df_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬取正负面评论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_comment(item_id, seller_id):\n",
    "    base_tag_url = r'https://rate.tmall.com/listTagClouds.htm?itemId=531979715453&isAll=true'\n",
    "    r = requests.get(base_tag_url)\n",
    "    r = json.loads(r.text[9:])\n",
    "\n",
    "    pos_tag_ids = []\n",
    "    neg_tag_ids = []\n",
    "    for tag_cloud in r['tagClouds']:\n",
    "        if tag_cloud['posi']:\n",
    "            pos_tag_ids.append(tag_cloud['id'])\n",
    "        else:\n",
    "            neg_tag_ids.append(tag_cloud['id'])\n",
    "\n",
    "    pos_tag_ids = []\n",
    "\n",
    "    f_pos = codecs.open(WORKSPACE + 'comment.pos', 'a', encoding='utf-8')\n",
    "    f_neg = codecs.open(WORKSPACE + 'comment.neg', 'a', encoding='utf-8')\n",
    "\n",
    "    for tag_id in pos_tag_ids:\n",
    "        f_pos.writelines('\\n'.join(get_one_sentiment_comment(\n",
    "            item_id, seller_id, tag_id, 1)))\n",
    "    for tag_id in neg_tag_ids:\n",
    "        f_neg.writelines('\\n'.join(get_one_sentiment_comment(\n",
    "            item_id, seller_id, tag_id, -1)))\n",
    "\n",
    "    f_pos.close()\n",
    "    f_neg.close()\n",
    "\n",
    "\n",
    "def get_one_sentiment_comment(item_id, seller_id, tag_id, posi):\n",
    "    print 'Getting tag id {} posi {}...'.format(tag_id, posi)\n",
    "#     comment_url = r'https://rate.tmall.com/list_detail_rate.htm?itemId={}&sellerId={}&order=1&currentPage=1&append=0&content=0&tagId={}&posi={}'.format(\n",
    "#         item_id, seller_id, tag_id, posi)\n",
    "\n",
    "#     r = requests.get(comment_url)\n",
    "#     r_json = json.loads(r.text[15:])\n",
    "\n",
    "    contents = []\n",
    "\n",
    "    # 翻页\n",
    "    page = 1\n",
    "    page_num = 1\n",
    "    while page <= page_num:\n",
    "        comment_url = r'https://rate.tmall.com/list_detail_rate.htm?itemId={}&sellerId={}&order=1&currentPage={}&append=0&content=0&tagId={}&posi={}'.format(\n",
    "            item_id, seller_id, page, tag_id, posi)\n",
    "\n",
    "        try:\n",
    "            r = requests.get(comment_url)\n",
    "            r_json = json.loads(r.text[15:])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        page_num = r_json['paginator']['lastPage']\n",
    "\n",
    "        for comment in r_json['rateList']:\n",
    "            contents.append(preprocess(comment['rateContent']))\n",
    "\n",
    "        page += 1\n",
    "    for content in contents:\n",
    "        print type(content), content\n",
    "    return contents\n",
    "\n",
    "\n",
    "# get_sentiment_comment(531979715453, 1770496755)\n",
    "# get_sentiment_comment(531823943112,1770496755)\n",
    "# get_sentiment_comment(521310943964,1681391303)\n",
    "get_sentiment_comment(529060341366,1735917344)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_proxy():\n",
    "    \n",
    "    proxies = {'http': 'http://127.0.0.1:8087',\n",
    "               'https': 'https://59.44.16.8:80'}\n",
    "#     r = requests.get('https://www.baidu.com', proxies=proxies)\n",
    "#     r = requests.get('http://www.qq.com', proxies=proxies)\n",
    "    r = requests.get('https://rate.tmall.com/list_detail_rate.htm?itemId=531979715453&sellerId=1770496755&order=3&currentPage=1&append=0&content=1', proxies=proxies)\n",
    "    print r, r.text\n",
    "\n",
    "\n",
    "# test_proxy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 描述性统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_comment_df():\n",
    "    comments_dir=r'G:/Data/NetEase/data/comments/'\n",
    "    comment_list=[]\n",
    "    for f in os.listdir(comments_dir):\n",
    "        comment_file=os.path.join(comments_dir,f)\n",
    "        df=pd.read_csv(comment_file)\n",
    "        if df.empty: continue\n",
    "        df['item_id']=f[:-4]\n",
    "        comment_list.append(df)\n",
    "\n",
    "    return pd.concat(comment_list)\n",
    "\n",
    "df_comment=get_comment_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一天评论数量变化趋势"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_comment['hour']=df_comment['rate_date'].map(lambda x:str(x)[-8:-6])\n",
    "\n",
    "hour_goup=df_comment.groupby('hour').count()['item_id']\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.bar(np.arange(24),hour_goup)\n",
    "plt.title('Comment Num per Hour')\n",
    "plt.xlabel('24 hours')\n",
    "plt.ylabel('Comment num')\n",
    "plt.show()\n",
    "\n",
    "# product_path=r\"G:\\Data\\NetEase\\data\\comments\\520363612568.csv\"\n",
    "# df_comment=pd.read_csv(product_path)\n",
    "# df_comment['rate_date']=pd.to_datetime(df_comment['rate_date'])\n",
    "\n",
    "# # df_comment.head()\n",
    "\n",
    "# df_comment.resample('M',on='rate_date').count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从图中可以看出用户评论的时间主要是在白天，晚上23点之后评论量开始急速下降。\n",
    "评论有两个高峰时段，分别是中午12点和晚上20点，这与日常人们的下班闲暇时间吻合。如果App需要推送商品推荐信息，则选择这两个时间段可能是一个很好的选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用户鞋码分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_shoe_size(text):\n",
    "    text = preprocess(text)\n",
    "    \n",
    "    # 无鞋码信息\n",
    "    if re.search(ur'\\d{2}',text)==None:\n",
    "        return ''\n",
    "    \n",
    "    try:\n",
    "        result=text.split(';')[1].split(':')[1]\n",
    "        result=re.sub('/.*','',result)\n",
    "        result=re.sub(u'码','',result)\n",
    "        result=re.sub(ur'[\\[\\(（].*[\\]\\)）]','',result)\n",
    "        return result\n",
    "    except:\n",
    "        r = re.findall('\\d{2}', text)\n",
    "        result = ''\n",
    "        if len(r) > 0:\n",
    "            result = r[0]\n",
    "        return result\n",
    "\n",
    "df_comment['shoe_size']=df_comment['auction_sku'].map(extract_shoe_size)\n",
    "shoe_size_group=df_comment.groupby('shoe_size').count()['item_id']\n",
    "\n",
    "x=np.arange(len(shoe_size_group))\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.bar(x,shoe_size_group)\n",
    "plt.title('Shoe Size Distribution')\n",
    "plt.xlabel('Shoe size')\n",
    "plt.ylabel('Comment num')\n",
    "plt.xticks(x,shoe_size_group.index)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从鞋码分布图来看，排行前三的是41码，42码和43码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用户颜色偏好分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_shoe_color(text):\n",
    "    text = preprocess(text)\n",
    "    result=re.findall(ur'[黑白赤橙黄绿青蓝紫红灰金银铜棕褐]',text)\n",
    "    result=list(set(result))\n",
    "    result.sort()\n",
    "    if len(result)==0:\n",
    "        result=[u'未知']\n",
    "    return ' '.join(result)\n",
    "# for text in df_comment['auction_sku']:\n",
    "#     print text\n",
    "#     print extract_shoe_color(text)\n",
    "\n",
    "df_comment['shoe_color']=df_comment['auction_sku'].map(extract_shoe_color)\n",
    "shoe_color_group=df_comment.groupby('shoe_color').count()['item_id'].sort_values(ascending=False)\n",
    "\n",
    "\n",
    "x=np.arange(len(shoe_color_group))\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.bar(x,shoe_color_group)\n",
    "plt.title('Shoe Color Distribution')\n",
    "plt.xlabel('Shoe color combination')\n",
    "plt.ylabel('Comment num')\n",
    "plt.xticks(x,shoe_color_group.index,rotation=70,fontproperties=fp_msyh)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从跑鞋颜色分布图来看，“黑 白”颜色组合的跑鞋成为最畅销的产品，且远远超过其它的颜色组合。其中“黑”色在销量排名前几位的颜色组合中几乎都可以看到它的影子，因此“黑”色是跑鞋设计中需要重点考虑的颜色。\n",
    "\n",
    "另外，蓝色、绿色和橙色也具有一定的用户群。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用户印象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "hownet_pos_opinion_words = [line.strip() for line in codecs.open(\n",
    "    WORKSPACE + \"HowNetPosOpinionWords.txt\", encoding='utf-8').readlines()]\n",
    "hownet_neg_opinion_words = [line.strip() for line in codecs.open(\n",
    "    WORKSPACE + \"HowNetNegOpinionWords.txt\", encoding='utf-8').readlines()]\n",
    "\n",
    "\n",
    "def compute_unit_polarity(unit):\n",
    "    p = 0\n",
    "    if unit['opinion'] in hownet_pos_opinion_words:\n",
    "        p = 1\n",
    "\n",
    "    if unit['opinion'] in hownet_pos_opinion_words and unit['negPosition'] == -1:\n",
    "        p = -1\n",
    "\n",
    "    if unit['opinion'] in hownet_neg_opinion_words:\n",
    "        p = -1\n",
    "\n",
    "    if unit['opinion'] in hownet_neg_opinion_words and unit['negPosition'] == -1:\n",
    "        p = 1\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "\n",
    "def grey_color_func(word, font_size, position, orientation, random_state=None,\n",
    "                    **kwargs):\n",
    "    return \"hsl(0, 0%%, %d%%)\" % random.randint(60, 100)\n",
    "\n",
    "# 绘制标签云\n",
    "def wordcloud(freqs_pos,freqs_neg):\n",
    "    freqs_pos=dict(freqs_pos)\n",
    "    freqs_neg=dict(freqs_neg)\n",
    "    \n",
    "#     freqs_pos=dict([(u'速度毋庸置疑',5),(u'数据准确',5),(u'脚掌大',5),(u'脚后跟疼',5),(u'老公喜欢',5),(u'尺寸合适',6),(u'软硬适中',5),(u'穿着舒服',5),(u'小段同学',4),(u'曲小花',3),(u'中文分词',2),(u'样例',1)])\n",
    "#     freqs_neg=dict([(u'速度毋庸置疑',5),(u'数据准确',5),(u'脚掌大',5),(u'脚后跟疼',5),(u'老公喜欢',5),(u'尺寸合适',6),(u'软硬适中',5),(u'穿着舒服',5),(u'小段同学',4),(u'曲小花',3),(u'中文分词',2),(u'样例',1)])\n",
    "    \n",
    "    shoe_mask = np.array(Image.open(WORKSPACE+\"mask.jpg\"))\n",
    "    \n",
    "    wc_pos=WordCloud(font_path=WORKSPACE+\"msyh.ttc\",background_color=\"white\",mask=shoe_mask)\n",
    "    wc_pos.generate_from_frequencies(freqs_pos,max_font_size=90)\n",
    "    \n",
    "    wc_neg=WordCloud(font_path=WORKSPACE+\"msyh.ttc\",background_color=\"white\",mask=shoe_mask)\n",
    "    wc_neg.generate_from_frequencies(freqs_neg,max_font_size=90)\n",
    "    \n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(wc_pos)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(wc_neg.recolor(color_func=grey_color_func, random_state=3),\n",
    "           interpolation=\"bilinear\")    \n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "opinion_units=[]\n",
    "def extract(words,tags,deps):\n",
    "    unit={'target':0,'opinion':0,'degree':0,'neg':0,'negPosition':0}\n",
    "    \n",
    "    # Rule 1\n",
    "    for idx,tag in enumerate(tags):\n",
    "        unit=unit.fromkeys(unit,0)\n",
    "        if tag=='VA':\n",
    "            unit['opinion']=words[idx]\n",
    "            for dep in deps:\n",
    "                # 评价对象\n",
    "                if dep[1]==idx+1 and dep[0]=='nsubj':\n",
    "                    unit['target']=words[dep[2]-1]\n",
    "                    \n",
    "                # 程度副词\n",
    "                if dep[1]==idx+1 and dep[0]=='advmod':\n",
    "                    unit['degree']=words[dep[2]-1]\n",
    "                \n",
    "                # 否定词\n",
    "                if dep[1]==idx+1 and dep[0]=='neg':\n",
    "                    unit['neg']=words[dep[2]-1]\n",
    "                    if dep[1]==dep[2]+1:\n",
    "                        unit['negPosition']=-1\n",
    "                    else:\n",
    "                        unit['negPosition']=-2\n",
    "            if unit['target']==0: unit['target']='Implicit'\n",
    "            opinion_units.append(unit)\n",
    "          \n",
    "    # Rule 2\n",
    "    for dep in deps:\n",
    "        unit=unit.fromkeys(unit,0)\n",
    "        if dep[0]=='nsubj':\n",
    "            if tags[dep[1]-1]=='VV' and tags[dep[2]-1]=='NN':\n",
    "                unit['target']=words[dep[2]-1]\n",
    "                unit['opinion']=words[dep[1]-1]\n",
    "                opinion_units.append(unit)\n",
    "\n",
    "                \n",
    "# product_path=r\"G:\\Data\\NetEase\\test.csv\"\n",
    "product_path=WORKSPACE+r\"data\\comments\\531979715453.csv\"\n",
    "df_comment=pd.read_csv(product_path)\n",
    "\n",
    "num=df_comment.shape[0]\n",
    "\n",
    "for comment_id,comment in enumerate(df_comment['content']):\n",
    "    \n",
    "#     if comment_id==100:\n",
    "#         break\n",
    "        \n",
    "    sys.stdout.write('\\r'+'Processing sentence {}/{}...'.format(comment_id,num))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    # text = ('非常舒服轻便。运动芯片也很赞。智能运动鞋超级酷。')\n",
    "    text=preprocess(comment).encode(\"utf-8\")\n",
    "\n",
    "    properties={'pinelineLanguage':'zh','annotators': 'tokenize,ssplit,pos,depparse','outputFormat': 'json'}\n",
    "    \n",
    "    r=requests.post('http://localhost:9000',params={'properties': str(properties)},data=text)  \n",
    "    r_json=json.loads(r.text)\n",
    "\n",
    "    for sentence in r_json['sentences']:\n",
    "        words=[]\n",
    "        tags=[]\n",
    "        deps=[]\n",
    "\n",
    "        for token in sentence['tokens']:\n",
    "    #         print token['word'],token['pos']\n",
    "            words.append(token['word'])\n",
    "            tags.append(token['pos'])\n",
    "        for dep in sentence['basicDependencies']:\n",
    "    #         print (dep['dep'],dep['governor'],dep['dependent'])\n",
    "            deps.append((dep['dep'],dep['governor'],dep['dependent']))\n",
    "\n",
    "        extract(words,tags,deps)\n",
    "\n",
    "# for p in opinion_units:\n",
    "#     print p['target'],p['opinion'],p['degree'],p['neg'],p['negPosition']\n",
    "\n",
    "# 计算情感单元情感倾向性    \n",
    "pos_opinion_units=[]\n",
    "neg_opinion_units=[]\n",
    "for unit in opinion_units:\n",
    "    p=compute_unit_polarity(unit)\n",
    "    if p==1:\n",
    "        pos_opinion_units.append(unit)\n",
    "    elif p==-1:\n",
    "        neg_opinion_units.append(unit)\n",
    "    else:\n",
    "        pass\n",
    "#         print 'Cant find polarity!',unit['target'],unit['opinion'],unit['degree'],unit['neg'],unit['negPosition']\n",
    "\n",
    "print 'Positive: ',len(pos_opinion_units),'Negative: ',len(neg_opinion_units)\n",
    "\n",
    "# 统计频率\n",
    "def units_to_freqs(opinion_units):\n",
    "    opinion_dict={}\n",
    "    for unit in opinion_units:\n",
    "        \n",
    "        to_append=''\n",
    "        if unit['degree']==0: unit['degree']=''\n",
    "        \n",
    "        if not unit['neg']==0:\n",
    "            if unit['negPosition']==-1:\n",
    "#                 print unit['degree'],unit['neg'],unit['opinion']\n",
    "                to_append=unit['degree']+unit['neg']+unit['opinion']\n",
    "            elif unit['negPosition']==-2:\n",
    "                to_append=unit['neg']+unit['degree']+unit['opinion']\n",
    "        else:\n",
    "            to_append=unit['degree']+unit['opinion']\n",
    "        \n",
    "        if opinion_dict.has_key(unit['target']):        \n",
    "            opinion_dict[unit['target']].append(to_append)\n",
    "        else:           \n",
    "            opinion_dict[unit['target']]=[to_append]\n",
    "\n",
    "    opinion_dict=dict([(key,dict(Counter(value)))for key,value in opinion_dict.items()])  \n",
    "    \n",
    "    # 处理隐式评价对象\n",
    "    opinion_dict[' ']=opinion_dict.pop('Implicit')\n",
    "\n",
    "    return sorted([(t+o,v) for t,value in opinion_dict.items() for o,v in value.items()],key=lambda x:x[1],reverse=True)    \n",
    "        \n",
    "pos_impression_freqs=units_to_freqs(pos_opinion_units)\n",
    "neg_impression_freqs=units_to_freqs(neg_opinion_units)\n",
    "\n",
    "wordcloud(pos_impression_freqs,neg_impression_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: 这里需要Stanford CoreNLP 3.7.0库的支持，下载后在文件夹内运行命令：\n",
    "\n",
    "`java -Xmx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -serverProperties StanfordCoreNLP-chinese.properties -port 9000 -timeout 15000`\n",
    "\n",
    "我们这里使用商品531979715453（https://detail.tmall.com/item.htm?id=531979715453 ）为例绘制用户印象标签云。\n",
    "\n",
    "查看其它商品的用户印象标签云，修改代码`product_path=WORKSPACE+r\"data\\comments\\531979715453.csv\"`即可。\n",
    "\n",
    "标签云可以让用户清楚地看到商品的优点和缺点，商品制造企业也可以根据用户的评论对产品的不足之处进行相应的改进。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 情感倾向性分析\n",
    "\n",
    "上述用户印象标签已经计算了用户评论的情感倾向性，使用了基于情感词典的计算方法。基于词典的计算方法不足之处在于情感词对于不同的评价对象或许有不同的情感极性，如情感词“轻”，描述鞋时情感为正，描述礼物时情感则为负。这里可以通过将问题看作分类问题，使用Word2Vec结合深度学习对评论句子进行情感倾向性判别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec词向量训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 时间序列分析（用户情感演化，购买量演化）\n",
    "## 竞品分析（卖的好的产品，卖的不好的产品，背后原因）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "119px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "556px",
    "left": "0px",
    "right": "1182px",
    "top": "106px",
    "width": "185px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
